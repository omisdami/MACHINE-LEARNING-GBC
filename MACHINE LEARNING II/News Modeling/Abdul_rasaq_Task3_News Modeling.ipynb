{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Modeling\n",
    "\n",
    "Topic modeling involves **extracting features from document terms** and using\n",
    "mathematical structures and frameworks like matrix factorization and SVD to generate **clusters or groups of terms** that are distinguishable from each other and these clusters of words form topics or concepts\n",
    "\n",
    "Topic modeling is a method for **unsupervised classification** of documents, similar to clustering on numeric data\n",
    "\n",
    "These concepts can be used to interpret the main **themes** of a corpus and also make **semantic connections among words that co-occur together** frequently in various documents\n",
    "\n",
    "Topic modeling can help in the following areas:\n",
    "- discovering the **hidden themes** in the collection\n",
    "- **classifying** the documents into the discovered themes\n",
    "- using the classification to **organize/summarize/search** the documents\n",
    "\n",
    "Frameworks and algorithms to build topic models:\n",
    "- Latent semantic indexing\n",
    "- Latent Dirichlet allocation\n",
    "- Non-negative matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "The latent Dirichlet allocation (LDA) technique is a **generative probabilistic model** where each **document is assumed to have a combination of topics** similar to a probabilistic latent semantic indexing model\n",
    "\n",
    "In simple words, the idea behind LDA is that of two folds:\n",
    "- each **document** can be described by a **distribution of topics**\n",
    "- each **topic** can be described by a **distribution of words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Algorithm\n",
    "\n",
    "- 1. For each document, **randomly initialize each word to one of the K topics** (k is chosen beforehand)\n",
    "- 2. For each document D, go through each word w and compute:\n",
    "    - **P(T |D)** , which is a proportion of words in D assigned to topic T\n",
    "    - **P(W |T )** , which is a proportion of assignments to topic T over all documents having the word W\n",
    "- **Reassign word W with topic T** with probability P(T |D)Â´ P(W |T ) considering all other words and their topic assignments\n",
    "\n",
    "![LDA](https://raw.githubusercontent.com/subashgandyer/datasets/main/images/LDA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "- Install the necessary library\n",
    "- Import the necessary libraries\n",
    "- Download the dataset\n",
    "- Load the dataset\n",
    "- Pre-process the dataset\n",
    "    - Stop words removal\n",
    "    - Email removal\n",
    "    - Non-alphabetic words removal\n",
    "    - Tokenize\n",
    "    - Lowercase\n",
    "    - BiGrams & TriGrams\n",
    "    - Lemmatization\n",
    "- Create a dictionary for the document\n",
    "- Filter low frequency words\n",
    "- Create an Index to word dictionary\n",
    "- Train the Topic Model\n",
    "- Predict on the dataset\n",
    "- Evaluate the Topic Model\n",
    "    - Model Perplexity\n",
    "    - Topic Coherence\n",
    "- Visualize the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Users/omisesan/anaconda3/lib/python3.11/site-packages/endtoendmlgit-0.0.1-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /Users/omisesan/anaconda3/lib/python3.11/site-packages/fonttools-4.55.2-py3.11-macosx-10.9-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: gensim in /Users/omisesan/anaconda3/lib/python3.11/site-packages (4.3.0)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.4-cp311-cp311-macosx_10_9_x86_64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: numpy>=1.24.2 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (1.24.3)\n",
      "Requirement already satisfied: scipy in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (1.12.0)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (2.0.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (1.2.0)\n",
      "Requirement already satisfied: jinja2 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (3.1.2)\n",
      "Requirement already satisfied: numexpr in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (2.8.4)\n",
      "Requirement already satisfied: funcy in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (1.3.0)\n",
      "Requirement already satisfied: setuptools in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (68.0.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from gensim) (5.2.1)\n",
      "Collecting FuzzyTM>=0.4.0 (from gensim)\n",
      "  Downloading FuzzyTM-2.0.9-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp311-cp311-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp311-cp311-macosx_10_9_x86_64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp311-cp311-macosx_10_9_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp311-cp311-macosx_10_9_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from spacy) (0.14.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from spacy) (1.10.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from spacy) (23.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting pyfume (from FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading pyFUME-0.3.4-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.0.0->pyLDAvis) (2.2.0)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.2)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from jinja2->pyLDAvis) (2.1.1)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Collecting scipy (from pyLDAvis)\n",
      "  Downloading scipy-1.10.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (53 kB)\n",
      "Collecting numpy>=1.24.2 (from pyLDAvis)\n",
      "  Downloading numpy-1.24.4-cp311-cp311-macosx_10_9_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting simpful==2.12.0 (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading simpful-2.12.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting fst-pso==1.8.1 (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pyfume to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyfume (from FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading pyFUME-0.3.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/omisesan/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Collecting miniful (from fst-pso==1.8.1->pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hDownloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading spacy-3.8.4-cp311-cp311-macosx_10_9_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp311-cp311-macosx_10_9_x86_64.whl (42 kB)\n",
      "Downloading FuzzyTM-2.0.9-py3-none-any.whl (31 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.12-cp311-cp311-macosx_10_9_x86_64.whl (26 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-macosx_10_9_x86_64.whl (132 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp311-cp311-macosx_10_9_x86_64.whl (635 kB)\n",
      "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m635.9/635.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.4-cp311-cp311-macosx_10_9_x86_64.whl (839 kB)\n",
      "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m839.3/839.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading blis-1.2.0-cp311-cp311-macosx_10_9_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyFUME-0.3.1-py3-none-any.whl (59 kB)\n",
      "Downloading marisa_trie-1.2.1-cp311-cp311-macosx_10_9_x86_64.whl (192 kB)\n",
      "Downloading simpful-2.12.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: fst-pso, miniful\n",
      "  Building wheel for fst-pso (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20430 sha256=8e793993bfdb2f6b857ab54483b9e71f0f5d5e731819f648f489ad84abdf894f\n",
      "  Stored in directory: /Users/omisesan/Library/Caches/pip/wheels/69/f5/e5/18ad53fe1ed6b2af9fad05ec052e4acbac8e92441df44bad2e\n",
      "  Building wheel for miniful (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3513 sha256=3defd9932249543cdb2714a6a96c556e3e63a835cf564a9bdc573191c79f7c11\n",
      "  Stored in directory: /Users/omisesan/Library/Caches/pip/wheels/9d/ff/2f/afe4cd56f47de147407705626517d68bea0f3b74eb1fb168e6\n",
      "Successfully built fst-pso miniful\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, simpful, preshed, miniful, language-data, langcodes, fst-pso, confection, weasel, thinc, pyfume, spacy, FuzzyTM, pyLDAvis\n",
      "Successfully installed FuzzyTM-2.0.9 blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 fst-pso-1.8.1 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 miniful-0.0.6 murmurhash-1.0.12 preshed-3.0.9 pyLDAvis-3.4.1 pyfume-0.3.1 simpful-2.12.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    }
   ],
   "source": [
    "! pip install pyLDAvis gensim spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset\n",
    "Dataset: https://raw.githubusercontent.com/subashgandyer/datasets/main/newsgroups.json\n",
    "\n",
    "#### 20-Newsgroups dataset\n",
    "- 11K newsgroups posts\n",
    "- 20 news topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['content', 'target', 'target_names'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open('newsgroups.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(type(data))  \n",
    "print(data.keys() if isinstance(data, dict) else \"Not a dictionary\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in data: dict_keys(['content', 'target', 'target_names'])\n",
      "Number of documents: 11314\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a m\n"
     ]
    }
   ],
   "source": [
    "with open('newsgroups.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Keys in data: {data.keys()}\")\n",
    "if isinstance(data[\"content\"], list):\n",
    "    data_text = data[\"content\"]\n",
    "elif isinstance(data[\"content\"], dict):\n",
    "    data_text = list(data[\"content\"].values())\n",
    "else:\n",
    "    raise ValueError(\"Unexpected format for 'content' key\")\n",
    "\n",
    "print(f\"Number of documents: {len(data_text)}\")\n",
    "print(data_text[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Email Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emails(texts):\n",
    "    return [re.sub(r'\\S*@\\S*\\s?', '', text) for text in texts]\n",
    "\n",
    "data_text = remove_emails(data_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newline Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newlines(texts):\n",
    "    return [re.sub(r'\\n', ' ', text) for text in texts]\n",
    "\n",
    "data_text = remove_newlines(data_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Quotes Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_quotes(texts):\n",
    "    return [re.sub(r\"\\'\", \"\", text) for text in texts]\n",
    "\n",
    "data_text = remove_single_quotes(data_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "- Create **sent_to_words()** \n",
    "    - Use **gensim.utils.simple_preprocess**\n",
    "    - Use **generator** instead of an usual function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp', 'posting', 'host', 'rac', 'wam', 'umd', 'edu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could']\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # Use gensim's simple_preprocess to tokenize and clean text\n",
    "        yield(simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data_text))\n",
    "print(data_words[0][:30])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words Removal\n",
    "- Extend the stop words corpus with the following words\n",
    "    - from\n",
    "    - subject\n",
    "    - re\n",
    "    - edu\n",
    "    - use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/omisesan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "data_words_nostops = remove_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove_stopwords( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_nostops = remove_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams\n",
    "- Use **gensim.models.Phrases**\n",
    "- 100 as threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wheres', 'thing', 'car', 'nntp_posting', 'host_rac', 'wam_umd', 'organization_university', 'maryland_college', 'park_lines', 'wondering_anyone', 'could_enlighten', 'car', 'saw', 'day', 'door_sports', 'car', 'looked', 'late_early', 'called', 'bricklin', 'doors', 'really', 'small', 'addition', 'front_bumper', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'production', 'car', 'made', 'history', 'whatever', 'info', 'funky_looking', 'car', 'please_mail', 'thanks', 'il', 'brought', 'neighborhood', 'lerxst']\n"
     ]
    }
   ],
   "source": [
    "bigram = gensim.models.Phrases(data_words_nostops, min_count=2, threshold=10)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "   \n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "print(data_words_bigrams[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make_bigrams( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_bigrams = make_bigrams(data_words_nostops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "- Use spacy\n",
    "    - Download spacy en model (if you have not done that before)\n",
    "    - Load the spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3mâ  As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "\u001b[33mDEPRECATION: Loading egg at /Users/omisesan/anaconda3/lib/python3.11/site-packages/endtoendmlgit-0.0.1-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /Users/omisesan/anaconda3/lib/python3.11/site-packages/fonttools-4.55.2-py3.11-macosx-10.9-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2mâ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lemmatizaton( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['s', 'thing', 'car', 'nntp_poste', 'wam_umd', 'could_enlighten', 'car', 'see', 'day', 'door_sport', 'car', 'look', 'late_early', 'call', 'door', 'really', 'small', 'addition', 'separate', 'rest', 'body', 'know', 'model', 'name', 'engine', 'spec', 'year', 'production', 'car', 'make', 'history', 'info', 'funky_looke', 'car', 'please_mail', 'thank', 'bring', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the dictionary: 80969\n"
     ]
    }
   ],
   "source": [
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "print(f\"Number of unique words in the dictionary: {len(id2word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus: 11314\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 5), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)]\n"
     ]
    }
   ],
   "source": [
    "texts = data_lemmatized\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "print(f\"Number of documents in corpus: {len(corpus)}\")\n",
    "print(corpus[0][:20])  # Print first 20 word-id pairs of first document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter low-frequency words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words after filtering: 17699\n"
     ]
    }
   ],
   "source": [
    "# Filter out words that occur in less than 5 documents, or more than 50% of the documents\n",
    "id2word.filter_extremes(no_below=5, no_above=0.5)\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "print(f\"Number of unique words after filtering: {len(id2word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Index 2 word dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of index to word mapping: [(0, 'addition'), (1, 'body'), (2, 'bring'), (3, 'call'), (4, 'car')]\n"
     ]
    }
   ],
   "source": [
    "id2word_dict = dict((id, word) for word, id in id2word.token2id.items())\n",
    "print(f\"Sample of index to word mapping: {list(id2word_dict.items())[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a News Topic Model\n",
    "\n",
    "#### LdaModel\n",
    "- **num_topics** : this is the number of topics you need to define beforehand\n",
    "- **chunksize** : the number of documents to be used in each training chunk\n",
    "- **alpha** : this is the hyperparameters that affect the sparsity of the topics\n",
    "- **passess** : total number of training assess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=10,\n",
    "    random_state=100,\n",
    "    update_every=1,\n",
    "    chunksize=100,\n",
    "    passes=10,\n",
    "    alpha='auto',\n",
    "    per_word_topics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the Keyword in the 10 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Keywords per Topic:\n",
      "Topic 0: 0.016*\"science\" + 0.012*\"truth\" + 0.011*\"research\" + 0.011*\"accept\" + 0.010*\"study\" + 0.010*\"earth\" + 0.009*\"world\" + 0.009*\"report\" + 0.009*\"describe\" + 0.008*\"reference\"\n",
      "Topic 1: 0.033*\"write\" + 0.025*\"get\" + 0.022*\"think\" + 0.021*\"say\" + 0.020*\"know\" + 0.018*\"go\" + 0.016*\"see\" + 0.015*\"make\" + 0.014*\"organization\" + 0.013*\"time\"\n",
      "Topic 2: 0.028*\"people\" + 0.015*\"evidence\" + 0.014*\"state\" + 0.011*\"fact\" + 0.010*\"say\" + 0.010*\"case\" + 0.010*\"law\" + 0.009*\"right\" + 0.009*\"issue\" + 0.009*\"person\"\n",
      "Topic 3: 0.018*\"year\" + 0.012*\"team\" + 0.011*\"game\" + 0.010*\"good\" + 0.010*\"first\" + 0.009*\"point\" + 0.009*\"line\" + 0.007*\"play\" + 0.007*\"_\" + 0.007*\"well\"\n",
      "Topic 4: 0.020*\"use\" + 0.016*\"line\" + 0.016*\"system\" + 0.014*\"program\" + 0.010*\"problem\" + 0.010*\"file\" + 0.010*\"also\" + 0.010*\"window\" + 0.009*\"need\" + 0.009*\"thank\"\n",
      "Topic 5: 0.027*\"key\" + 0.016*\"chip\" + 0.014*\"sale\" + 0.012*\"bike\" + 0.010*\"tape\" + 0.010*\"ripem\" + 0.009*\"use\" + 0.009*\"com\" + 0.009*\"system\" + 0.009*\"ride\"\n",
      "Topic 6: 0.019*\"patient\" + 0.017*\"copy\" + 0.014*\"post\" + 0.013*\"regard\" + 0.012*\"type\" + 0.011*\"non\" + 0.010*\"list\" + 0.010*\"cause\" + 0.009*\"serious\" + 0.009*\"color\"\n",
      "Topic 7: 0.025*\"reason\" + 0.021*\"claim\" + 0.018*\"man\" + 0.016*\"believe\" + 0.016*\"faith\" + 0.016*\"book\" + 0.014*\"sense\" + 0.014*\"exist\" + 0.012*\"religion\" + 0.011*\"true\"\n",
      "Topic 8: 0.017*\"new\" + 0.017*\"car\" + 0.014*\"drive\" + 0.012*\"space\" + 0.012*\"price\" + 0.011*\"power\" + 0.010*\"sell\" + 0.009*\"win\" + 0.009*\"cost\" + 0.008*\"model\"\n",
      "Topic 9: 0.023*\"gun\" + 0.010*\"fire\" + 0.010*\"city\" + 0.009*\"kill\" + 0.009*\"drug\" + 0.009*\"carry\" + 0.009*\"police\" + 0.008*\"black\" + 0.008*\"steal\" + 0.008*\"shoot\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 Keywords per Topic:\")\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic {idx}: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Topic Models\n",
    "- Model Perplexity\n",
    "- Topic Coherence\n",
    "\n",
    "### Model Perplexity\n",
    "\n",
    "Model perplexity is a measurement of **how well** a **probability distribution** or probability model **predicts a sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -8.761123945294319\n"
     ]
    }
   ],
   "source": [
    "print(f\"Perplexity: {lda_model.log_perplexity(corpus)}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00015670837792431427\n"
     ]
    }
   ],
   "source": [
    "actual_perplexity = np.exp(-8.761123945294319)\n",
    "print(actual_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Coherence\n",
    "Topic Coherence measures score a single topic by measuring the **degree of semantic similarity** between **high scoring words** in the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.40410622495073856\n"
     ]
    }
   ],
   "source": [
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_lda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Topic Model\n",
    "- Use **pyLDAvis**\n",
    "    - designed to help users **interpret the topics** in a topic model that has been fit to a corpus of text data\n",
    "    - extracts information from a fitted LDA topic model to inform an interactive web-based visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for visualization\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, n_jobs=1)\n",
    "pyLDAvis.display(vis)\n",
    "\n",
    "# Optional: Save visualization to HTML\n",
    "pyLDAvis.save_html(vis, 'Abdulrasaq_lda_visualization.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
